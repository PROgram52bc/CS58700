\documentclass{article}

%\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{url}

%\usepackage{dsfont}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{listings}

\newcommand{\class}{ CS58700 Spring 2025 }
\newcommand{\website}{{\tiny\url{https://www.cs.purdue.edu/homes/ribeirob/courses/Spring2025}}}
\newcommand{\homeworknumber}{1}
\newcommand{\duedate}{ {\bf 11:59pm}, Tuesday, February 18th (open until 4:00am next day)}
\newcommand{\code}{}

%\pagestyle{headings}
\setlength{\parskip}{1pc}
\setlength{\parindent}{0pt}
\setlength{\topmargin}{-1pc}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\newcommand{\var}{\mbox{var}}
\newcommand{\cov}{\mbox{cov}}

\newcommand{\newpart}{
\stepcounter{partno}
\noindent
{\bf (\alph{partno})}
}

\newcommand{\header}{
\newpage
\noindent
\framebox{ \vbox{\class Homework \hfill --- Homework \homeworknumber --- \hfill  Last update: \today
  \\ \website \hfill {\color{red} Due: \duedate} }}
\bigskip
\newline
{\bf Instructions and Policy:} Each student should write up their own solutions independently, no copying of any form is allowed.
You MUST to indicate the names of the people you discussed a problem with; ideally you should
discuss with no more than two other people.\\
{\color{red} YOU MUST INCLUDE YOUR NAME IN THE HOMEWORK}\\
You need to submit your answer in PDF. {\LaTeX }  is typesetting is encouraged but not required.  Please write clearly and concisely - clarity and
brevity will be rewarded. Refer to known facts as necessary. 
\newline
}

\newcounter{questionno}
\setcounter{questionno}{-1}
\newcounter{partno}

\newcommand{\question}[1]{
\noindent
\newline
\stepcounter{questionno}
\setcounter{partno}{0}
{\bf Q\arabic{questionno} (#1 pts): }
}

\begin{document}

\header

\question{{\color{red}0pts correct answer,  -1,000pts incorrect answer: (0,-1,000)}} 
A correct answer to the following questions is worth 0pts. An incorrect answer is worth -1,000pts, which carries over to  other homeworks and exams, and can result in an F grade in the course.
\begin{enumerate}[(1)]
\item Student interaction with other students / individuals:
\begin{enumerate}[(a)]
\item I have copied part of my homework from another student or another person (plagiarism).
\item \textbf{Yes, I discussed the homework with another person but came up with my own answers. Their name(s) is (are): Jason Fotso}
\item No, I did not discuss the homework with anyone
\end{enumerate}
\item On using online resources:
\begin{enumerate}[(a)]
\item I have copied one of my answers directly from a website (plagiarism).
\item \textbf{I have used online resources to help me answer this question, but I came up with my own answers (you are allowed to use online resources as long as the answer is your own). Here is a list of the websites I have used in this homework:\\ \url{https://pytorch.org/docs/main}}
\item I have not used any online resources except the ones provided in the course website.
\end{enumerate}


\end{enumerate}

\newpage

\noindent\textbf{Learning Objectives}: Let students understand deep learning tasks, basic feedforward neural network properties, backpropagation, and invariant representations.

\hfill

\noindent \textbf{Learning Outcomes}: After you finish this homework, you should be capable of explaining and implementing feedforward neural networks with arbitrary architectures or components from scratch.


%-----------------------------------------------

\subsection*{Concepts}

\question{2.0} 
In what follows we give a paper and ask you to classify the paper into tasks.\\ 

{\bf Mark ALL that apply and EXPLAIN YOUR ANSWERS. Answers without explanations will get deducted -0.05 points.}\\
~\\
{\bf Note 1:} This includes marking both a specific answer and its more general counterpart. E.g., Covariate shift adaptation is also a type of Domain adaptation. Your answer explanation can help assign partial credits. \\
~\\
{\bf Note 2:} {\em Papers may describe multiple tasks. Please make sure you describe which task you focused on in ``Explain Your Answers''.}. \\
~\\
{\color{red}
{\bf Note 3:} {\em Do not take the author's words for granted!} Many papers claim they are doing one task (e.g. OOD generalization), but in reality, their methods are really doing another task (e.g. domain adaptation or few-shot learning). Use your own judgment and make sure that the task can be formally cast into one of the tasks discussed in the lecture.
}~\\

{\bf Point distribution:}
\begin{itemize}
	\item Each question starts with 0.5 points.
	\item Each missing task counts as -0.1 (should be marked but was not).
	\item Each extra task counts as -0.1 (was marked but should not).
	\item Each MARKED answer not explained in ``Explain Your Answers'' gets deducted -0.05. Items left unmarked need not be explained.
		\begin{itemize}
			\item Example of an explanation: The image task in the paper is a supervised learning task: the training data is $\{(x_i,y_i)\}_i$, where $x_i$ is an image and $y_i$ is the image's label. The data $\{(x_i,y_i)\}_i$ is assumed independent, where the train and test distributions are assumed to be the same. The learning is transductive since the test data is provided during training in the form of an extra dataset where...
		\end{itemize}   
	
	\item The minimum score is zero.
\end{itemize}   

\newpage
\begin{enumerate}
\item (0.5) Kipf, Thomas N., and Max Welling.``Semi-Supervised Classification with Graph Convolutional Networks." In International Conference on Learning Representations, 2017.
\begin{enumerate}[(a)]
\item Independent observations
\item \textbf{Dependent observations} The nodes are dependent on each other through the connections in the graph structure.
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item \textbf{Semi-supervised learning} It uses semi-supervised learning with unlabeled graph as input to do node classification.
\item \textbf{In-distribution test data} The training data and the test data are from the same distribution.
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item \textbf{Associational task (i.e., not causal)} No causal reasoning is involved in the graph node classification process.
\item Causal task
\end{enumerate}
Explain your answers\\
 
\newpage

\item (0.5) Zhao, Jianan, Hesham Mostafa, Mikhail Galkin, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. "Graphany: A foundation model for node classification on any graph." arXiv preprint arXiv:2405.20445 (2024).
\begin{enumerate}[(a)]
\item Independent observations
\item \textbf{Dependent observations}
The attention scores capture dependency between input data (graph nodes).
\item Supervised learning
\item Unsupervised learning
\item Self-supervised learning
\item \textbf{Semi-supervised learning} It uses a mix of labeled and unlabeled data for training.
\item \textbf{In-distribution test data} Although the test data are unseen, they are assumed to have the same distribution.
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item \textbf{Inductive  learning}
The predictions ensure generalization to new graphs.
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item \textbf{Associational task (i.e., not causal)} No causal relationship is involved in the node classification task.
\item Causal task
\end{enumerate}

\newpage

\item (0.5) Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. “Attention is All you Need.” Neural Information Processing Systems (2017).
\begin{enumerate}[(a)]
\item Independent observations
\item \textbf{Dependent observations}

The observations are dependent because word tokens depend on previous seen
tokens, by the attention scores.

\item Supervised learning
\item Unsupervised learning
\item \textbf{Self-supervised learning}

The paper uses self-supervised learning because the training data consists of a
large corpus of unlabeled text, where the correlation is used for updating the
weights.

\item Semi-supervised learning
\item In-distribution test data
\item Out-of-distribution test data
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item Covariate shift adaptation
\item Target shift adaptation
\item Domain adaptation
\item \textbf{Associational task (i.e., not causal)}

The task is an associational task, because it only observes the correlation
between tokens, and there are no causal connection between the words.

\item Causal task
\end{enumerate}
Explain your answers

\newpage

\item (0.5) S Chandra Mouli, Bruno Ribeiro, ``Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks'', International Conference on Learning Representations, 2022. 
\begin{enumerate}[(a)]
\item \textbf{Independent observations}
It is independent observations, because the task carried out is a classification task, where inputs are independently draw from the distribution.

\item Dependent observations
\item \textbf{Supervised learning}
It is supervised learning because the training process of the classification involved labeled input.
\item Unsupervised learning
\item Self-supervised learning
\item Semi-supervised learning
\item In-distribution test data
\item \textbf{Out-of-distribution test data}
It uses out-of-distribution test data, because the goal of the paper is to find a model that will fit the test data (of a different distribution from the train data).
\item Transfer learning
\item Transductive learning
\item Inductive  learning
\item \textbf{Covariate shift adaptation}
Because the input domain is transformed by an unknown transformation $X'$.
\item Target shift adaptation
\item Domain adaptation
\item Associational task (i.e., not causal)
\item \textbf{Causal task}
The counterfactuals in Layer 3 involves causal reasoning on the transformation of the input.
\end{enumerate}
Explain your answers\\
\end{enumerate}

\newpage
\question{3.0} Please answer the following questions \textbf{concisely} but with enough details to get partial credit if the answer is incorrect. 

\begin{enumerate}


\item (0.5) For neural network models, regularization terms are usually applied on weight parameters. Describe why we do not regularize the bias term in the model? \\
{\bf Hint:} Biases are activation thresholds. Assume a very large regularization on the bias. Now assume an input of all zeros.

The goal of bias is to prevent overfitting, but bias does not interact with the
input so does not contribute to overfitting. Also, with smaller or close-to-zero
inputs, regularizing bias will make the model struggle to fit non-zero output
values.

\vspace{3in}

\item (0.5)  Learning feedforward networks with ReLUs: Could a ReLU activation cause problems when learning a model with gradient descent? Let $\{X_i\}_{i=1}^N,X_i\in \mathbb{R}^d$ be the training data. Let $W_j^{(1)}\in \mathbb{R}^d$ be the $j$-th neuron weight in the first layer. Give a non-trivial subset $\mathcal{W}$ where $\forall W_j^{(1)}\in \mathcal{W}$ the gradient $\frac{\partial L(\{X_i\}_{i=1}^N)}{\partial W_j^{(1)}}$ is zero. \\
{\bf Hint:} We say a neuron is {\em dead} when its output is constant for all training examples. \\

A non-trivial subset $\mathcal{W}$ would satisfy the constraint 

$$\{W \mid \forall X \in \{X_i\}_{i=1}^N , W \cdot X + b < 0 \}$$

where $b$ is a constant bias term.

\newpage

\item (0.5) 
Suppose the symmetrization (Reynolds operator) $\Bar{T}$ of a finite linear transformation group $G$ has rank $0$. Prove that any $G$-invariant neuron has just the bias term.\\
{\bf Hint:} For example, a zero matrix has rank $0$.

Since a matrix of zero rank is a matrix that has no linearly-independent columns by definition, it can only be a zero matrix. With a zero matrix as the Reynolds operator, we have

$$\sigma(W^T x + b) = \sigma(W^T \bar{T} x + b)$$

where $\bar{T} = \textbf{0}$. 

Since $\textbf{0} x = x$ for any vector $x$, we have

$$\sigma(W^T x + b) = \sigma(b)$$

So the neurons in the G-invariant layer has just the bias term.


\item (0.5)  Consider a supervised learning task, where the training data is $(Y,X) \sim P^\text{tr}(Y,X)$ and $A \sim b$ means random variable $A$ is sampled from distribution $b$. Consider two finite linear transformation groups $G_1$ and $G_2$. Let $f_i: \mathbb{R}^d \to [0,1]$ be single neuron that is $G_i$-invariant, for $i=1,2$. Describe how we could test (and be sure) that $f_1$ is also invariant to $G_2$ or $f_2$ is also invariant to $G_1$. \textit{Assume we only have access to the neuron weights ${\bf w_i}$ of $f_i$, $i=1,2$ and the Reynolds operator $\overline{T}_1$ and $\overline{T}_2$ for the groups $G_1$ and $G_2$, respectively.} \\
{\bf Hint 1:} Just testing the $f$'s with some transformed inputs will not guarantee they are invariant to all inputs and all transformations.\\
{\bf Hint 2:} Pay attention to the invariant subspace that defines the parameters of $f_1$ and $f_2$, which forces the neurons to be $G_1$- and $G_2$-invariant, respectively.

By the definition of the Reynold's operator, we can simply test whether the following are true:

$$\overline{T}_2(\mathbf{w}_1) = \mathbf{w}_1$$
$$\overline{T}_1(\mathbf{w}_2) = \mathbf{w}_2$$

If they are true, then we know that they are also invariant to the other subspace, and vise versa.

\newpage

\item (1.0) Consider the neurons $f_1$ and $f_2$ of the previous question. For each of the groups $i=1,2$, let $\bar{T}_i$ be the symmetrization (Reynolds) operator of group $G_i$. We also have a set of left eigenvectors vectors
${\bf v}_{i,k}^T \bar{T}_i = {\bf v}_{i,k}^T$, $k=1,\ldots,K$, for each of the groups. Explain how to create a neuron that is both $G_1$- and $G_2$-invariant. Prove that this neuron is invariant to any composition of transformations from both $G_1$ and $G_2$, such as $T_1 T_2 T_1'$, $T_1,T_1' \in G_1$, $T_2 \in G_2$.
%

With the two Reynolds operators $\bar{T}_1$ and $\bar{T}_2$, we solve for equation

$$\bar{T}_1 \mathbf{w} = \mathbf{w}, \quad \bar{T}_2 \mathbf{w} = \mathbf{w}$$.

If there is a solution for $\mathbf{w}$, then it is a neuron that is invariant to both subspaces.

To show

$$\mathbf{w}^T T_1 T_2 T_1' \mathbf{x} = \mathbf{w}^T \mathbf{x}$$,

we first show 

$\mathbf{w}^T T_1 = \mathbf{w}^T$.

We know that 

$\mathbf{w}^T \bar{T_1} = \mathbf{w}^T$, by definition of Reynold's operator, which means

$$\frac{1}{|G_1|} \sum_{T_1 \in G_1} \mathbf{w}^T T_1 = \mathbf{w}^T$$
implying
$$\frac{1}{|G_1|} \sum_{T_1 \in G_1} \mathbf{w}^T T_1 - \mathbf{w}^T = 0$$
implying
$$\frac{1}{|G_1|} \sum_{T_1 \in G_1} \mathbf{w}^T (T_1 - \mathbf{w}^T)^2 = 0$$

Since the squared term $(T_1 - \mathbf{w}^T)^2$ must be greater than or equal to
$0$, we know that for every transformation $T_1$ in group $G_1$, we have 

$\mathbf{w}^T T_1 = \mathbf{w}^T$.


Thus, 

$$\mathbf{w}^T T_1 T_2 T_1' \mathbf{x} = \mathbf{w}^T T_2 T_1' \mathbf{x}$$

the same reduction can be done for $G_2$ using the same logic, which allows us to derive 

$$\mathbf{w}^T T_1 T_2 T_1' \mathbf{x} = \mathbf{w}^T \mathbf{x}$$.


\vspace{3in}

\end{enumerate}

%-----------------------------------------------

\newpage
\subsection*{Programming (5.0 pts)}

Throughout this semester you will be using Python and PyTorch as the main tool
to complete your homework, which means that getting familiar with them is
required.  PyTorch (\url{http://pytorch.org/tutorials/index.html}) is a
fast-growing Deep Learning toolbox that allows you to create deep learning
projects on different levels of abstractions, from pure tensor operations to
neural network blackboxes. The official tutorial and their github repository are
your best references. Please make sure you have the latest stable version on the
machine.  Linux machines with GPU installed are suggested. Moreover, following
PEP8 coding style is recommended.

\hfill

\noindent \textbf{Skeleton Package}: A skeleton package is available at \textbf{Brightspace}.
 You should download it and use the folder structure provided. In some homework, skeleton code might be provided. If so, you should based on the prototype to write your implementations.

\subsection*{Introduciton to PyTorch}

PyTorch, in general, provides three modules, from high-level to low-level abstractions, to build up neural networks. We are going to study 3 specific modules in this homework. First, the module that provides the highest abstraction is called \textbf{torch.nn}. It offeres layer-wise abstraction so that you can define a neural layer through a function call. For example, \textbf{torch.nn.Linear(.)} creates a fully connected layer. Coupling with contains like \textbf{Sequential(.)}, you can connect the network layer-by-layer and thus easily define your own networks. The second module is called \textbf{torch.AutoGrad}. It allows you to compute gradients with respect to all the network parameters, given the feedforward function definition (the objective function). It means that you don't need to analytically compute the gradients, but only need to define the objective function while coding your networks. Note that in most cases, we don't need to directly import methods from torch.AutoGrad. Most of the time, it is sufficient to call \textbf{loss.backward()}, which calls torch.AutoGrad functionalities under the hood and computes the backpropagation gradients for us. The last module we are going to use is \textbf{torch.tensor} which provides effecient ways of conducting tensor operations or computations so that you can customize your network in the low-level. The official PyTorch has a thorough tutorial to this (\url{http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#}). You are required to go through it and understand all three modules well before you move on.

\subsubsection*{HW Overview}

In this homework, you are going to implement vanilla feedforwardneural networks on a couple of different ways. The overall submission should be structured as below:

\hfill

\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{tikzpicture}[%
  grow via three points={one child at (0.5,-0.7) and
  two children at (0.5,-0.7) and (0.5,-1.4)},
  edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
  \node {bruno\_ribeiro\_hw\homeworknumber}
    child { node [selected] {my\_neural\_networks}
      child { node {\_\_init\_\_.py}}
      child { node {networks.py}}
      child { node {activations.py}}
      child { node {mnist.py}}
      child { node {minibatcher.py}}
      child { node [optional] {any\_others.py}}
    }
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child [missing] {}
    child { node {ReadMe}}
    child { node {hw\homeworknumber\_training.py}}
    child { node {hw\homeworknumber\_demo.py}}
    child { node {hw\homeworknumber\_learning\_curves.py}}
    child { node [selected] {numpy\_mlp.py}}
    child { node [selected] {G-equivariant\_Auto-Encoder}
      child { node {G-equivariant-AE-skeleton.ipynb}}
      child { node {example-G-equi-ae-output.png}}
      child { node {example-ae-output.png}}
      child { node {example\_image.png}}
      };
\end{tikzpicture}

\hfill

\begin{itemize}
\item \textbf{bruno\_ribeiro\_hw\homeworknumber}: the top-level folder that contains all the files
          required in this homework. You should replace the file name with your
          name and follow the naming convention mentioned above.

\item \textbf{ReadMe}: Your ReadMe should begin with a couple of \textbf{example commands}, e.g., ``python hw\homeworknumber.py data", used to generate the outputs you report. TA would replicate your results with the commands
          provided here. More detailed options, usages and designs of your
          program can be followed. You can also list any concerns that you
          think TA should know while running your program. Note that put the
          information that you think it's more important at the top. Moreover,
          the file should be written in pure text format that can be displayed
          with Linux ``less" command.

\item \textbf{hw\homeworknumber\_training.py}: One executable we prepared for you to run
          training with your networks.

\item \textbf{hw\homeworknumber\_learning\_curves.py}: One executable for training models
          and plotting learning curves.

\item \textbf{hw\homeworknumber\_learning\_demo.py}: Demonstrate some basic Python packages. Just FYI.

\item \textbf{my\_neural\_networks}: Your Python neural network package.
          The package name in this homework is \textbf{my\_neural\_networks},
          which should NOT be changed while submitting it. Two modules should
          be at least included:
\begin{itemize}
\item \textbf{networks.py}
\item \textbf{activations.py}
\end{itemize}
Except these two modules, a package constructor \textbf{\_\_init\_\_.py} is also required for importing your modules. You are welcome to architect the package in your own favorite. For instance, adding another module, called utils.py, to facilitate your implementation.

Two additional modules, \textbf{mnist.py} and \textbf{minibatcher.py}, are also attached, and are used in the main executable to load the dataset and create minibatches (which is not needed in this homework.). You don't need to do anything with them.

\item \textbf{numpy\_mlp.py}: Your numpy-only two-layer MLP model with residual connection. The training skeleton code is provided, but you will need to implement the forward, backward, and SGD update all in numpy.

\item \textbf{G-equivariant\_Auto-Encoder}: Follow the python notebook inside for constructing a G-equivariant Auto-Encoder step-by-step.

\end{itemize}


\subsubsection*{Data: MNIST}

You are going to conduct a simple classification task, called MNIST. It classifies images of hand-written digits (0-9). Each example thus is a \(28 \times 28\) image. 
\begin{itemize}
\item The full dataset contains 60k training examples and 10k testing examples.
\item We provide a data loader (read\_images(.) and read\_labels(.) in \textbf{my\_neural\_networks/mnist.py}) that will automatically download the data.
\end{itemize}


\subsubsection*{\bf Warm-up: Implement Activations}

\noindent Open the file \textbf{my\_neural\_networks/activations.py}.
As a warm up activity, you are going to implement the
\textbf{activations} module, which should realize activation functions and
objective functions that will be used in your neural networks. Note that whenever you see "raise NotImplementedError", you should implement it.

Since these
functions are mathematical equations, the code should be pretty short
and simple. The main intuition of this section is to help you get familiar
with basic Python programming, package structures, and test cases. As an
example, a Sigmoid function is already implemented in the module. Here are
the functions that you should complete:
\begin{itemize}
\item \textbf{relu}: Rectified Linear Unit (ReLU), which is defined as
  \begin{equation*}
    a_k^l = relu(z_k^l) =
    \begin{cases}
      0       & \quad \text{if } z_k^l < 0\\
      z_k^l  & \quad \text{otherwise }.
    \end{cases}
\end{equation*}

\item \textbf{softmax}: the basic softmax
  \begin{equation}
  a_k^L = softmax(z_k^L) = \frac{e^{z_k^L}}{\sum_{c} e^{z_c^L}},
  \end{equation}

\item \textbf{stable\_softmax}: the \textbf{numerically stable softmax}. You should test
if this outputs the same result as the basic softmax.
\begin{align}
softmax(x_i) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
&= \frac{C e^{x_i}}{C \sum_j e^{x_j}} \\
&= \frac{e^{x_i+\log C}}{\sum_j e^{x_j + log C}}
\end{align}
A common choice for the constant is $log C = -\max_{j} x_j$.

\item \textbf{average cross\_entropy}:
\begin{equation}
E = - \frac{1}{\text{sizeof}(\text{mini-batch})}\sum_{d \in  \text{mini-batch}} t_d \log a_k^L = - \frac{1}{\text{sizeof}(\text{mini-batch})} \sum_{d \in \text{mini-batch}} t_d (z_d^L - \log \sum_c e^{z_c^L}).
\end{equation}
where \(d\) is a data point; \(t_d\) is its true label; \(a_k^L\) is the
propability predicted by the network.

\end{itemize}

\noindent \textbf{Hints}: Make sure you tested your implementation with corner
cases before you move on. Otherwise, it would be hard to debug.

\subsubsection*{Warm-up: Understand Example Network}

\noindent Open the files \textbf{hw1\_training.py} and \textbf{my\_neural\_networks/example\_networks.py}.

\hfill

\noindent \textbf{hw1\_training.py} is the main executable (trainer). It controls in a high-level view. The task is called MNIST, which classifies images of hand-written digits. The executable uses a class called \textbf{TorchNeuralNetwork} fully implemented in \textbf{my\_neural\_networks/example\_networks.py}.

\hfill

\noindent In this task, you don't need to write any codes, but only need to play with the modules/executables provided in the skeleton and answer qeustions. A class called \textbf{TorchNeuralNetwork} is fully implemented in \textbf{my\_neural\_networks/example\_networks.py}. You can run the trainer with it by feeding correct arguments into \textbf{hw\homeworknumber\_training.py}. Read through all the related code and write down what is the correct command ("python hw\homeworknumber\_training.py" with arguments) to train such example networks in the report.

\hfill

\noindent Here is a general summary about each method in the \textbf{TorchNeuralNetwork}.
\begin{itemize}
    \item \textbf{\_\_init\_\_(self, shape, gpu\_id=-1)}: the constructor that takes
      network shape as parameters. The network weights are declared as matrices in this method.
      You should not make any changes to them, but need to think about how to use them
      to do vectorized implementations.
      \begin{itemize}
          \item Your implementation should support arbitrary network shape, rather
            than a fixed one. The shape is in specified in tuples. For exapmles,
            "shape=(784, 100, 50, 10)" means that the numbers of neurons in the
            input layer, first hidden layer, second hidden layer, and output layer
            are 784, 100, 50, and 10 respectively.
          \item All the hidden layers use \textbf{ReLU} activations.
          \item The output layer uses \textbf{Softmax} activations.
          \item \textbf{Cross-Entropy} loss should be used as the objective.
      \end{itemize}
    \item \textbf{train\_one\_epoch(self, X, y, y\_1hot, learning\_rate)}:
      conduct network training for one epoch over the given data \textbf{X}.
      It also returns the loss for the epoch.
      \begin{itemize}
          \item this method consists of three important components: feedforward, backpropagation, and weight updates.
          \item (Non-stochastic) \textbf{Gradient descent} is used. The gradient calculatation should base on all the input data. However, this part is given.
      \end{itemize}
    \item \textbf{predict(self, X)}: predicts labels for \textbf{X}.
\end{itemize}

\hfill

\noindent You need to understand the entire skeleton well at this point. \textbf{TorchNeuralNetwork} should give you a good starting point to understand all the method semantics, and the \textbf{hw\homeworknumber\_training.py} should demonstrate the training process we want. In the next task, you are going to implement another two classes supporting the same set of methods. The inputs and outputs for the methods are the same, while the internal implementations have different constrains. Therefore, make sure you understand all the method semantics and inputs/outputs before you move on.

\question{1}{\bf Implement Feedforward Neural Network with Autograd}

\noindent Open the file \textbf{my\_neural\_networks/networks.py}.

\hfill

\noindent The task here is to complete the class \textbf{AutogradNeuralNetwork}. In your implementation, several constrains are enforced:
\begin{itemize}
  \item You are NOT allowed to use any high-level neural network modules,
    such as torch.nn, unless it is specified. No credits will be given if
    similar packages or modules are used.
  \item You need to follow the methods prototypes given in the skeleton. This
    contrain might be removed in the future. However, as the first homework,
    we want you to know what do we expect you to complete in a PyTorch project.
  \item You should leave at least the \textbf{hw\homeworknumber\_training.py} untouched in the final submission.
  During grading, we will replace whatever you have with the original \textbf{hw\homeworknumber\_training.py}.
\end{itemize}

\noindent For \textbf{AutogradNeuralNetwork}, you only need to complete the \textbf{feedforward part}. Other parts should already be given in the skeleton. You should be able to run the \textbf{hw\homeworknumber\_training.py} in a way similar to what you discovered in the last task. Specifically, what you need to is as follows:

\begin{itemize}
  \item Understand semantics of all the class members (parameters declared via \textbf{torch.nn.Parameters()}), especially the few defined in the constructor.
  \item Identify the codes related three main components for training: feedforward, backpropagation, and weight updates.
  \item The second and third components are given. Only the \textbf{feedforward} is left for you, so go ahead and complete the \textbf{\_feed\_forward()} method.
\end{itemize}

\newpage

\noindent \textbf{Things to be included in the report}:
\begin{enumerate}
  \item command line arguments for running this experiment with \textbf{hw\homeworknumber\_training.py}. \\

  \item Specify network shape as (784, 300, 100, 10). Collect results for \textbf{100 epochs} .
        Make two plots: "Loss vs. Epochs" and "Accuracy vs. Epochs". The
        accuracy one should include results for both training and
        testing data. Analyze and compare each plot generated in the last step. Write down your observations.
        \includegraphics{accuracy_Q3.png}
        \\For accuracy, it increases as the number of epochs increases. The test accuracy is slightly higher than train accuracy.
        \\
        \includegraphics{loss_Q3.png}
        \\The loss decreases as the epoch number increases.

\end{enumerate}


\noindent \textbf{Hints}:
\begin{itemize}
    \item The given skeleton has all the input/output definitions. Please read through it, and if you found any typos or unclear parts, feel free to ask.
    \item In general, you don't need to change any codes given in the skeleton, unless it is for debugging.
    \item Feel free to define any helper functions/modules you need.
    \item You might need to figure out how to conduct vectorized implementations so that the pre-defined members can be utilized in a succinct and efficient way.
    \item You are welcome to use GPUs to accelerate your program
    \item For debugging, you might want to load less amount of training data to save time. This can done easily by make slight changes to \textbf{hw\homeworknumber\_training.py}.
    \item For debugging, you might want to explore some features in a Python package called \textbf{pdb}.
\end{itemize}


\newpage
\question{1}{Learning Curves: Deep vs Shallow}

\noindent Take a look at the trainer file called \textbf{hw\homeworknumber\_learning\_curves.py}

\hfill

\noindent This executable has very similar structure to the \textbf{hw\homeworknumber\_training.py}, but you are
going to vary training data size to plot learning curves introduced in the
lecture. Specifically, you need to do the followings:
\begin{enumerate}
  \item Load MNIST data: \url{http://yann.lecun.com/exdb/mnist/} into torch
        tensors
  \item Use \textbf{AutogradNeuralNetwork}.
  \item Vary training data size in the range between 250 to 10000 in increments of 250.
  \item Train and select a model for each data size. You need to design an \textbf{early
        stop} strategy to select the model so that the learning curves will be correct.
  \item Plot learning curves for training and testing sets with
    \begin{enumerate}
      \item a network shape (784, 10)
      \item a network shape (784, 300, 100, 10)
    \end{enumerate}
\end{enumerate}

\hfill
\newpage

\begin{enumerate}
  \item command line arguments for running this experiment with \textbf{hw\homeworknumber\_training.py}. \\
    \begin{lstlisting}[language = Python]
python3 ... --early_stop=patience --shape_config=2
python3 ... --early_stop=patience --shape_config=1
python3 ... --early_stop=threshold --shape_config=2
python3 ... --early_stop=threshold --shape_config=1
    \end{lstlisting}

   The above four commands are for : patience early stopping, shape (a),
   patience early stopping, shape (b), default (threshold) early stopping, shape
   (a), and default early stopping, shape (b) respectively.
   
  \item The early stop strategy you used in selecting models.

  I chose to implement the patience early stopping, where consecutive small
  improvements are required before early stopping takes place. As a result, the
  learning curves are more smooth than the default early stopping strategy.

  \item Specify network shape as (784, 300, 100, 10). Collect results for \textbf{100 epochs} .
        Make two plots: "Loss vs. Epochs" and "Accuracy vs. Epochs". The
        accuracy one should include results for both training and
        testing data. Analyze and compare each plot generated in the last step. Write down your observations.

  \item The 2 learning curve plots for the 2 network shapes.\\
        \includegraphics{learning_curve_patience_1.png}
        Patience early stopping with shape (a)\\
        \includegraphics{learning_curve_patience_2.png}
        Patience early stopping with shape (b)\\
        \includegraphics{learning_curve_threshold_1.png}
        Threshold (default) early stopping with shape (a)\\
        \includegraphics{learning_curve_threshold_2.png}
        Threshold (default) early stopping with shape (b)\\
    \item Analyze and compare each plot generated in the last step. Write down
          your observations.

          For shape (a), the training accuracy decreases as the training set
          size increases, it could be due to overfitting when the training size
          is too small, where the model is not generalizing to the test data. As
          the training set size increases, the accuracy converges. For network
          shape (a), there are less hidden layers, which could also contributed
          to a lower converging accuracy, because the model has limited number
          of neurons and layers to learn the distribution.

          For shape (b), both training and test accuracy increases as the
          training set size increases, because it helps the model to capture the
          pattern better. But the rate of the increase in accuracy decreases,
          due to the diminishing return on the benefit of the additional data.
          Note that because of the additional neurons, the model is able to
          generalize better, as indicated by the proximity between the training
          and test accuracies.

    \end{enumerate}


\noindent \textbf{Hints}: You should understand the information embedded in
    the learning curves and what it should look like. If your implementation
    is correct, you should be able to see meaningful differences.


\newpage
\question{2.0}{Backpropagation with Residual MLP from Scratch}

\noindent Open the file \textbf{numpy\_mlp.py}. 

Implement \textbf{NumpyResMLP} as a two-layer MLP with one residual connection, but you can NOT use \textbf{torch} components. That is, you need to implement the entire \textbf{NumpyResMLP} class, including forward propagation, backpropagation, and weight updates, all in \textbf{numpy}. For the backpropagation, you need to analytically compute the gradients.

The two-layer residual MLP is defined as follows. Specifically, in each forward pass, we are given a mini-batch of input $\mathbf{X} \in \mathbb{R}^{B \times d}$ of batch size $B$ and dimension $d$, with ground-truth labels vector $\mathbf{y} \in \{1, 2, \dots, C\}^B$ with $C$ classes. We denote $\text{AppendOnes}(\cdot)$ as the operation of appending a column of all-ones vector $\mathbf{1}_d$ of size $d$, to the right side of $\mathbf{X}$. That is, for $\overline{\mathbf{X}} = \text{AppendOnes}(\mathbf{X}) \in \mathbb{R}^{B \times (d + 1)}$, we have $\overline{\mathbf{X}}_i = [\mathbf{X}_{i, 1}, \dots, \mathbf{X}_{i, d}, 1]$ for every row $\overline{\mathbf{X}}_i$. (Can you tell what is the purpose of appending ones? This technique is covered in lecture.)

The input passes through forward pass as follows:

1. First layer with $\mathbf{W}^{(1)} \in \mathbb{R}^{(d+1) \times d}$:
\begin{align*}
\overline{\mathbf{X}} &= \text{AppendOnes}(\mathbf{X}) , \\
\mathbf{Z}^{(1)} &= \overline{\mathbf{X}}\mathbf{W}^{(1)} , \\
\mathbf{H}^{(1)} &= \max(0, \mathbf{Z}^{(1)}) \tag*{\text{ReLU activation}} .
\end{align*}

2. Residual (second) layer with $\mathbf{W}^{(2)} \in \mathbb{R}^{(d+1) \times d}$:
\begin{align*}
\mathbf{\overline{H}}^{(1)} &= \text{AppendOnes}(\mathbf{H}^{(1)}) , \\
\mathbf{Z}^{(2)} &= \mathbf{\overline{H}}^{(1)}\mathbf{W}^{(2)} ,\\
\mathbf{H}^{(2)} &= \max\left(0, \underbrace{\mathbf{Z}^{(2)} + \mathbf{H}^{(1)}}_{\text{Residual connection}}\right) .
\end{align*}

3. Output layer with $\mathbf{W}^{(3)} \in \mathbb{R}^{(d + 1) \times C}$:
\begin{align*}
\mathbf{\overline{H}}^{(2)} &= \text{AppendOnes}(\mathbf{H}^{(2)}) , \\
\mathbf{Z}^{(3)} &= \mathbf{\overline{H}}^{(2)}\mathbf{W}^{(3)} ,\\
\mathbf{\hat{Y}} &= \mathbf{Z}^{(3)} - \log\left(\sum_{j=1}^{C} \exp(\mathbf{Z}^{(3)}_{:, j})\right) \tag*{Log-Softmax for log probabilities} ,
\end{align*}
where $\mathbf{Z}^{(3)}_{:, j}$ is the $j$-th column of $\mathbf{Z}^{(3)}$, and $\log(\cdot)$ and $\exp(\cdot)$ are the elementwise logarithmic and exponential function respectively.


4. Negative log-likelihood loss (NLL):
\begin{align*}
\mathcal{L} (\mathbf{X}, \mathbf{y}) = \frac{1}{B} \sum_{n=1}^B - \mathbf{\hat{Y}}_{n, \mathbf{y}_n} ,
\end{align*}
    
where $\mathbf{y}_n$ is the true label index of the $n$-th example in the batch, and accordingly $\mathbf{\hat{Y}}_{n, \mathbf{y}_n}$ is the model's log-probability for the true label for the $n$-th example.

So each layer takes its input to compute logits with weight matrix $\mathbf{W}$ and bias vector $\mathbf{b}$, then activation. We use negative log-likelihood loss (NLL) and vanilla stochastic gradient descent (SGD) without momentum or weight decay. 

\textbf{Questions:}
\begin{enumerate}

    \item (0.5) Based on the given forward pass, derive and write down all the
    mathematical equations used in your backpropagation implementation. You
    should take batch size $B$ into consideration when computing gradients.
    Importantly, your answer should include the gradients for the output logits,
    i.e. $\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(3)}}$, and gradients
    for the parameters of all intermediate layers, e.g.
    $\frac{\partial\mathcal{L}}{\partial\mathbf{W}^{(i)}}$. You may also show
    your steps with intermediate gradients, e.g.
    $\frac{\partial\mathcal{L}}{\partial\mathbf{\hat{Y}}}$ and
    $\frac{\partial\mathcal{L}}{\partial\mathbf{\mathbf{A}^{(i)}}}$, for partial
    credits. 
    
    {\bf Hint:} Start from deriving the gradients for the output logits,
    $\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(3)}}$. The final equation
    should not look very complex.
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(3)}} = \frac{1}{B} (\mathbf{Y}^{\text{one-hot}} - \exp(\mathbf{\hat{Y}}))
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(3)}} = \frac{1}{B} \mathbf{\overline{H}}^{(2) T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(3)}}
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^{(2)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(3)}} \mathbf{W}^{(3) T}
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{\Sigma}^{(2)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{H}^{(2)}} \odot (\mathbf{\Sigma}^{(2)} > 0)
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(2)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{\Sigma}^{(2)}}
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(2)}} = \mathbf{\overline{H}}^{(1)T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(2)}}
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{H}^{(1)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(2)}} + \frac{\partial \mathcal{L}}{\partial \mathbf{\Sigma}^{(2)}}
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(1)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{H}^{(1)}} \odot (\mathbf{Z}^{(1)} > 0)
\]

\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(1)}} = \mathbf{\overline{X}}^T \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{(1)}}
\]
    
    \item (0.5) You are now ready to implement \textbf{NumpyResMLP} with the expressions you have derived. Define and initialize your layer matrices and vectors in \textbf{\_\_init\_\_()}, and implement \textbf{forward()}, \textbf{backward()} of the model. Zero-initialization is fine, but you are encouraged to try He initialization too (no credit for this). The training code in \textbf{main()} is already provided for you, which takes care of loading the data, loading model, computing loss, and bookkeeping. However, study this code carefully as you would need to modify it for question 4. \\
    
    
    \item (0.5) Use the default hidden size of 512, batch size of 128, and train for 30 epochs. This configuration is already specified in \textbf{main()}. The command line arguments for running the script is 
    \begin{lstlisting}[language = Python]
    python numpy_mlp.py --hidden-size 512 --batch-size 128 --epochs 30
    \end{lstlisting}

    Provide two plots in your report PDF: ''Loss vs Epochs'' and ''Accuracy vs
    Epochs''. The accuracy one should include results for both training and
    testing data. Analyze and compare each plot generated and write down your
    observations.

    \includegraphics[width=\textwidth]{numpy_results.png}

    The training loss decreases as more epochs are gone through, which indicates
    that the gradients are minimizing the loss gradually.

    The accuracy increases across the epochs, which indicates that the model is
    capturing the characteristics of the distribution through the learning
    process.  The test accuracy does not increase as much as training data, due
    to the model's limited ability to generalize to unseen data.

    \item (0.5) Study the \textbf{main()} function in both \textbf{numpy\_mlp.py} and \textbf{hw1\_learning\_curves.py}, and implement the \textbf{main\_learning\_curve()} function in \textbf{numpy\_mlp.py} to support creating learning curves for this model.  This time, vary the training data size in the range between 1000 and 56000 in increments of 5500, i.e., \textbf{range(5500, 56001, 5500)}. Try with 2 different hidden sizes, \textbf{128 and 512}. The other instructions are similar to what is in Q4. 
    \textbf{Things should be included in the report}:
      \begin{itemize}
        \item The early stop strategy you used in selecting models.
        \item The 2 learning curve plots for the 2 network shapes (hidden sizes of 128 and 512).
        \item Analyze and compare each plot generated in the last step. Write down
              your observations in the report PDF.
      \end{itemize}
    
\end{enumerate}

\newpage
\question{1.0}{Implement an G-Equivariant Autoencoder}

Open the python notebook file
\textbf{G-equivariant\_Auto-Encoder/G-equivariant-AE-skeleton.ipynb}. Follow the instruction step-by-step to implement an G-equivariant Auto-Encoder that is equivariant to transformations in the group formed by 90$^o$ rotations (of bounded squared images). 

Consider an input in $\mathbb{R}^m$, and the output in $\mathbb{R}^k$. Let ${\bf W} \in \mathbb{R}^{k \times m}$ be the neuron parameters. G-equivariance requires that transforming the input is the same as transforming the output: $
{\bf x} \in \mathbb{R}^m, \forall g \in G , \quad \rho_2(g) {\bf W} {\bf x} =  {\bf W} \rho_1(g) {\bf x},
$ where $\rho_1:G \to \mathbb{R}^{m \times m}$ and $\rho_2:G \to \mathbb{R}^{k \times k}$. Since the above is true for all ${\bf x}$, then  $\rho_2(g) {\bf W} \rho_1(g)^{-1} =  {\bf W},$ or equivalently  
$$\forall g \in G, \qquad \rho_2(g) \otimes \rho_1(g^{-1})^T \text{vec}({\bf W})  =  \text{vec}({\bf W}),$$ 
where vec flattens the matrix into a vector, and so the whole transformation is $\rho_2(g) \otimes \rho_1(g^{-1})^T = \rho_{12}(g)$.

\begin{enumerate}
    \item (0.25)  Describe all the transformations in your transformation group {\bf (in the PDF)}. For instance, $T^{90}$ the transformation that rotates the image counter-clockwise by 90 degrees must of course be present.\\

$T^{360}$ rotates the image to its original position. Its inverse is itself.

$T^{90}$ rotates the image by 90 degrees counter-clockwise. Its inverse is
$T^{270}$, which rotates the image by 270 degrees counter-clockwise.

$T^{180}$ rotates the image by 180 degrees counter-clockwise. Its inverse is itself.

$T^{270}$ rotates the image by 270 degrees counter-clockwise. Its inverse is
$T^{90}$, which rotates the image by 90 degrees counter-clockwise.

    \vspace{2in}
    \item (0.25)  Can we obtain $G$-equivariant weight matrix $W\in \mathbb{R}^{k\times m}$ w.r.t to the rotation group for any input ($m$) and output dimension ($k$)? Is there any requirement for $k$ and $m$?\\
    I think we need to make sure that $k > m$, because there can be multiple ways to project from a smaller input space to a larger space.
    \vspace{3in}
    \item (0.5) Follow the python notebook and fill in the missing codes step-by-step to implement an G-equivariant Auto-Encoder and show its performance both in in-distribution tasks and out-of-distribution (rotated) tasks. Can you explain a bit about what the dimension of the bias parameter in the G-equivariant layers should be and how choosing dimensions for the bias parameter could possibly break the equivariance property, e.g., will setting the bias parameter dimension to be the output dimension create problems?
    
\end{enumerate}
\newpage
\subsection*{Submission Instructions}

Please read the instructions carefully. Failed to follow any part might incur some score deductions.

\hfill

\subsection*{\bf PDF upload}

The report PDF must be uploaded on Gradescope (see link on Brightspace)

\hfill


\subsection*{\bf Code upload}

\noindent \textbf{Naming convention}: [firstname]\_[lastname]\_hw\homeworknumber

All your submitting code files, a ReadMe, should be included in one folder. The folder should be named with the above naming convention. For example, if my first name is "Bruno" and my last name is "Ribeiro", then for Homework \homeworknumber, my file name should be ``bruno\_ribeiro\_hw\homeworknumber''.


\hfill

\noindent \textbf{Tar your folder}: [firstname]\_[lastname]\_hw\homeworknumber.tar.gz

Remove any unnecessary files in your folder, such as training datasets. Make sure your folder structured as the tree shown in Overview section. Compress your folder with the the command: \textbf{tar czvf bruno\_ribeiro\_hw\homeworknumber.tar.gz czvf bruno\_ribeiro\_hw\homeworknumber} .

\hfill

\noindent \textbf{Submit}: 
Please submit through Brightspace. There is an option to upload the code of hw\homeworknumber.



%-----------------------------------------------



\end{document}